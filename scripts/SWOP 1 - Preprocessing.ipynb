{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf5a115-173e-4568-b0d1-4afadb1ed539",
   "metadata": {},
   "source": [
    "# SWOP script 1\n",
    "## Single  Subject Preprocessing\n",
    "\n",
    "Analysis script associated with the manuscript: ***Native word order processing is not uniform: An ERP-study of verb-second word order***, by Susan Sayehli, Marianne Gullberg, Aaron Newman, and Annika Andersson. (2022). *Frontiers in Psychology - Language Sciences*. DOI:[10.3389/fpsyg.2022.668276](https://www.frontiersin.org/articles/10.3389/fpsyg.2022.668276).\n",
    "\n",
    "This script reads in raw, continuous EEG data from each subject and applies the following preprocessing steps:\n",
    "1. Bandpass filter raw continuous data using a highpass cutoff of 1 Hz and a lowpass cutoff of 30 Hz (this is used for ICA only)\n",
    "1. Apply independent components analysis (fastICA) to the filtered data, generating as many components as required to explain 99% of the variance in the data\n",
    "1. Identify independent components associated with ocular artifacts using MNE's automated correlation approach, with a threshold of *z* > 3\n",
    "1. Bandpass filter the raw, continuous data with a finite impulse response zero-phase hamming-windowed filter, using a highpass cutoff of 0.1 Hz and a lowpass cutoff of 30 Hz\n",
    "1. Apply ICA correction to the 0.1-30 Hz filtered data to remove ocula artifacts\n",
    "1. Segment the continuous data into epochs time-locked to the onset of critical words, including a 1 s pre-stimulus baseline period and a 1 s post-stimulus period. Epochs are mean-centered, i.e., the mean amplitude over the entire 2 s window is subtracted from each epoch, separately at each channel\n",
    "1. Import log files from each participant's EEG session and add as metadata to the segmented ERP data\n",
    "1. Export preprocessed epochs to .fif format\n",
    "\n",
    "---\n",
    "Copyright 2016-21  [Aaron J Newman](https://github.com/aaronjnewman), [NeuroCognitive Imaging Lab](http://ncil.science), [Dalhousie University](https://dal.ca)\n",
    "\n",
    "Released under the [The 3-Clause BSD License](https://opensource.org/licenses/BSD-3-Clause)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a166a-9cd8-4197-9558-e6c2a12cb314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import mne\n",
    "mne.set_log_level('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd83ef-5a5a-498b-ad32-7ae18bac16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Parameters\n",
    "data_path = '../data/'\n",
    "\n",
    "subjects = ['s_04nm',  's_07ba',  's_09lo',  's_12wg',  's_13ff',  's_14mc',\n",
    "            's_15rj',  's_17oh',  's_18ak',  's_19am',  's_21ma',  's_23nj',\n",
    "            's_24zk',  's_25ks',  's_26nm',  's_27lm',  's_28js',  's_29ld',\n",
    "            's_30la',  's_31bf']\n",
    "\n",
    "# standard montage file to look up channel locations\n",
    "montage_fname = 'standard_1005'\n",
    "\n",
    "# Map event codes to labels\n",
    "event_id = {'V2/kanske/pronoun':222, 'V3/kanske/pronoun':122,\n",
    "            'V2/kanske/noun':224,    'V3/kanske/noun':124,\n",
    "            'V2/hemma/pronoun':232,  'V3/hemma/pronoun':132,\n",
    "            'V2/hemma/noun':234,     'V3/hemma/noun':134,\n",
    "            'V2/idag/pronoun':242,   'V3/idag/pronoun':142,\n",
    "            'V2/idag/noun':244,      'V3/idag/noun':144\n",
    "           }\n",
    "\n",
    "# event_id = {'V2/kanske':[222, 224], 'V3/kanske':[122, 124],\n",
    "#             'V2/hemma':[232, 234],  'V3/hemma':[132, 134],\n",
    "#             'V2/idag':[242, 244],   'V3/idag':[142, 144],\n",
    "#            }\n",
    "\n",
    "\n",
    "# specify parameters for epoching\n",
    "tmin = -1.0  # start of each epoch (in sec)\n",
    "tmax =  1.0  # end of each epoch (in sec)\n",
    "baseline = None\n",
    "reject = None\n",
    "flat = dict(eeg=5e-6)\n",
    "detrend = 0\n",
    "threshold = 75e-6\n",
    "\n",
    "# Filter cutoffs and other parameters\n",
    "l_freq = 0.1\n",
    "l_freq_ica = 1.\n",
    "h_freq = 30.0\n",
    "\n",
    "# maximum number of ICs to reject in ICA artifact correction\n",
    "ica_random_state = 42  # seed so ICA is reproducable each time it's run\n",
    "\n",
    "# Specify n_components as a decimal to set % explained variance\n",
    "n_components = .99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4738e224-043c-4f19-a31a-80c5ef042c56",
   "metadata": {},
   "source": [
    "## Loop over subjects and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f8a0e-f0e8-42bc-b0d8-c5856446a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    print(subject + '...')\n",
    "    ## Read raw data\n",
    "    raw = mne.io.read_raw_cnt(data_path + subject + '.cnt', eog='auto')\n",
    "\n",
    "    # We also need to rename a few channels to conform to capitalization in 10-20 montage\n",
    "    ch_rename = {'FZ':'Fz', 'CZ':'Cz', 'PZ':'Pz',\n",
    "                 'FP1':'Fp1', 'FP2':'Fp2',\n",
    "                }\n",
    "    mne.rename_channels(raw.info, ch_rename)\n",
    "    raw.set_montage(montage_fname)\n",
    "\n",
    "    # Filter\n",
    "    raw_filt = raw.load_data().copy().filter(l_freq, h_freq,\n",
    "                                             picks = mne.pick_types(raw.info, eeg=True, eog=True),\n",
    "                                             n_jobs = 12);\n",
    "\n",
    "    ## Artifact Detection & Correction with ICA\n",
    "\n",
    "    ### Filter data for ICA\n",
    "    # ICA performs better when low-frequency drift is removed with a 1 Hz highpass filter.\n",
    "    # After identifying artifacts, we will apply corrections to the 0.1 H filtered  data.\n",
    "    raw_ica = raw.load_data().copy().filter(l_freq_ica, h_freq,\n",
    "                                             picks = mne.pick_types(raw.info, eeg=True, eog=True),\n",
    "                                             n_jobs = 12);\n",
    "\n",
    "    ### Find ICA decomposition of data\n",
    "    ica = mne.preprocessing.ICA(n_components=n_components, random_state=ica_random_state, max_iter='auto')\n",
    "    ica.fit(raw_ica)\n",
    "\n",
    "    ### Idenfity ICA components associated with ocular artifacts\n",
    "    ica.exclude = []\n",
    "    # find which ICs match the EOG pattern\n",
    "    eog_indices, eog_scores = ica.find_bads_eog(raw_ica, threshold=3.)\n",
    "    ica.exclude = eog_indices\n",
    "   \n",
    "    ### Apply ICA corretions to data\n",
    "    raw_postica = ica.apply(raw_filt.copy())\n",
    "\n",
    "    ## Epoching\n",
    "    ### Extract event codes  to events array\n",
    "    all_codes = raw.annotations.to_data_frame()['description'].unique()\n",
    "    event_map = dict(zip(all_codes, all_codes.astype('int')))\n",
    "    events, event_id_new = mne.events_from_annotations(raw, event_id=event_map)\n",
    "\n",
    "    # ### Segment data into Epochs\n",
    "    epochs = mne.Epochs(raw_postica,\n",
    "                        events, event_id=event_id,\n",
    "                        tmin=tmin, tmax=tmax,\n",
    "                        baseline=baseline, detrend=detrend,\n",
    "                        on_missing='ignore',\n",
    "                        preload=True\n",
    "                        )\n",
    "\n",
    "    # One subject had 10 extra trials at the start we need to remove\n",
    "    # (false start?)\n",
    "    if subject == 's_07ba':\n",
    "        epochs = epochs[10:]\n",
    "\n",
    "    ### Import log files and add as metadata\n",
    "    # To be used later in mixed effects analyses\n",
    "    df = pd.read_csv(data_path + 'SWOP_log_files.csv', encoding='utf8')\n",
    "    # s_12 is missing the first epoch in the EEG data\n",
    "    if subject == 's_12wg':\n",
    "        epochs.metadata = df[(df['Subject'] == subject) & (df['Condition'].isin(event_id.values()))][1:]\n",
    "    # subj 28 is missing one event randomly relative to metadata. Hard-coded based on looking at the data\n",
    "    elif subject == 's_28js':\n",
    "        epochs.metadata = df[(df['Subject'] == subject) & (df['Condition'].isin(event_id.values()))].drop(index=8061)\n",
    "    else:\n",
    "        epochs.metadata = df[(df['Subject'] == subject) & (df['Condition'].isin(event_id.values()))]\n",
    "    epochs.drop_bad()\n",
    "\n",
    "    ## Export preprocessed epochs to file\n",
    "    # output file names - set to follow MNE conventions\n",
    "    epochs_fname = data_path + subject + '-epo.fif'\n",
    "    epochs.save(epochs_fname, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
